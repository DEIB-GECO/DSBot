# en_wf.yaml

## Where the samples will be written
save_data: wf/run/example
## Where the vocab(s) will be written
src_vocab: wf/run/example.vocab.src
tgt_vocab: wf/run/example.vocab.tgt
# Prevent overwriting existing files in the folder
overwrite: False

# Corpus opts:
data:
    corpus_1:
        path_src: wf/src-train2.txt
        path_tgt: wf/tgt-train2.txt
    valid:
        path_src: wf/src-val.txt
        path_tgt: wf/tgt-val.txt
# Vocabulary files that were just created
src_vocab: wf/run/example.vocab.src
tgt_vocab: wf/run/example.vocab.tgt

# this means embeddings will be used for both encoder and decoder sides
both_embeddings: ./wf/glove_new_50.txt
# to set src and tgt embeddings separately:
#src_embeddings: ./wf/try_glove_new.txt
#tgt_embeddings: ./wf/try_glove_new.txt

# supported types: GloVe, word2vec
embeddings_type: "word2vec"

# word_vec_size need to match with the pretrained embeddings dimensions
#word_vec_size: 300

# Train on a single GPU
world_size: 1
#gpu_ranks: [0]

# Where to save the checkpoints
save_model: wf/run/model1
save_checkpoint_steps: 1000
train_steps: 10000
valid_steps: 1000
enc_layers: 1
dec_layers: 1
#heads: 4
learning_rate: 1
rnn_size: 250
rnn_type: 'LSTM'
word_vec_size: 50
dropout: [0]
start_decay_step: 5000
decay_steps: 2000
##decay_method: 'noam'
#encoder_type: brnn
#decoder_type:
#decoder_type: transformer_lm
#position_encoding: true
#batch_size: 256
#dropout_steps: [0]
#global_attention: 'dot'
#attention_dropout: [0.1]
#enc_layers: 2
#dec_layers: 1
#rnn_size: 100